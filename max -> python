// 1. ReLU 클래스: ReLU 연산을 수행하는 클래스. 입력 값이 0보다 크면 그대로 출력하고, 그렇지 않으면 0으로 설정한다.
// 2. MaxPool 클래스: Max Pooling 연산을 수행하는 클래스. relu_out 입력 배열을 받아, 각 윈도우(패치) 내에서 최대 값을 선택한다.
// 3. 데이터 시뮬레이션: 각 Convolution Layer의 출력을 시뮬레이션하는 무작위 데이터를 생성한다.
// 4. ReLU 및 MaxPool 적용: 각 Convolution Layer의 출력에 대해 ReLU와 Max Pooling을 적용한다.
// 5. 출력 모양 확인: 각 단계의 출력 모양을 출력하여 확인한다.



import numpy as np

class ReLU:
    def __init__(self, conv_bit=12):  # conv_bit는 12bit로 설정
        self.conv_bit = conv_bit

    def forward(self, conv_out):
        relu_out = np.maximum(0, conv_out)
        return relu_out

class MaxPool:
    def __init__(self, conv_bit=12, pool_size=2):
        self.conv_bit = conv_bit
        self.pool_size = pool_size

    def forward(self, relu_out):
        # Assuming relu_out shape is (channels, height, width)
        channels, height, width = relu_out.shape
        out_height = height // self.pool_size
        out_width = width // self.pool_size

        pooled = np.zeros((channels, out_height, out_width))
        for i in range(out_height):
            for j in range(out_width):
                window = relu_out[:, i*self.pool_size:(i+1)*self.pool_size, j*self.pool_size:(j+1)*self.pool_size]
                pooled[:, i, j] = np.max(window, axis=(1, 2))

        return pooled

# Simulation data for each layer
conv1_out = np.random.randint(-2048, 2047, (32, 26, 34))  # (채널 수, 이미지 높이, 이미지 너비)
conv2_out = np.random.randint(-2048, 2047, (64, 13, 17))
conv3_out = np.random.randint(-2048, 2047, (128, 6, 8))

# Instantiate ReLU and MaxPool
relu1 = ReLU(conv_bit=12)
maxpool1 = MaxPool(conv_bit=12, pool_size=2)

relu2 = ReLU(conv_bit=12)
maxpool2 = MaxPool(conv_bit=12, pool_size=2)

relu3 = ReLU(conv_bit=12)
maxpool3 = MaxPool(conv_bit=12, pool_size=2)

# Apply ReLU and MaxPool to the first convolution layer output
relu1_out = relu1.forward(conv1_out)
maxpool1_out = maxpool1.forward(relu1_out)

# Apply ReLU and MaxPool to the second convolution layer output
relu2_out = relu2.forward(conv2_out)
maxpool2_out = maxpool2.forward(relu2_out)

# Apply ReLU and MaxPool to the third convolution layer output
relu3_out = relu3.forward(conv3_out)
maxpool3_out = maxpool3.forward(relu3_out)

# Print output shapes for verification
print("ReLU 1 Output Shape:", relu1_out.shape)
print("MaxPool 1 Output Shape:", maxpool1_out.shape)
print("ReLU 2 Output Shape:", relu2_out.shape)
print("MaxPool 2 Output Shape:", maxpool2_out.shape)
print("ReLU 3 Output Shape:", relu3_out.shape)
print("MaxPool 3 Output Shape:", maxpool3_out.shape)


// 위의 코드 수행 결과

// ReLU 1 Output Shape: (32, 26, 34)
// MaxPool 1 Output Shape: (32, 13, 17)
// ReLU 2 Output Shape: (64, 13, 17)
// MaxPool 2 Output Shape: (64, 6, 8)
// ReLU 3 Output Shape: (128, 6, 8)
// MaxPool 3 Output Shape: (128, 3, 4)
